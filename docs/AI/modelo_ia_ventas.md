# Modelo de PredicciÃ³n de Ventas (Random Forest Regressor)

**Autor:** Nataly  
**Fecha:** Noviembre 2025  
**PropÃ³sito:** DocumentaciÃ³n tÃ©cnica para defensa de proyecto

---

## ğŸ“Š Resumen Ejecutivo

Este documento explica el modelo de Machine Learning implementado en **SmartSales365** para predecir ventas futuras por categorÃ­a de producto. El modelo utiliza **Random Forest Regressor** y alcanza un **RÂ² Score de ~97%**, demostrando alta precisiÃ³n en las predicciones.

---

## 1. Â¿Por QuÃ© Random Forest?

### Razones de la ElecciÃ³n:

#### âœ… **Robusto ante datos no lineales**
Las ventas no siguen patrones lineales simples. Random Forest captura relaciones complejas entre:
- Estacionalidad (ventas altas en noviembre-diciembre)
- CategorÃ­as de productos (Blusas venden mÃ¡s que Jackets)
- Tendencias anuales (crecimiento 2023 â†’ 2024 â†’ 2025)

#### âœ… **Maneja mÃºltiples features sin necesidad de normalizaciÃ³n**
Random Forest no requiere que las variables estÃ©n en la misma escala:
```python
# Features pueden tener rangos muy diferentes:
aÃ±o: 2023-2025
mes: 1-12
cat_Blusas: 0 o 1 (binario)
```

#### âœ… **Interpretabilidad**
A diferencia de redes neuronales (cajas negras), Random Forest proporciona:
- **Feature Importance**: QuÃ© variables son mÃ¡s importantes
- **FÃ¡cil debugging**: Puedes inspeccionar Ã¡rboles individuales
- **Explicable al negocio**: "El modelo dice que la categorÃ­a es lo mÃ¡s importante"

#### âœ… **Buen rendimiento en datos tabulares**
Estudios demuestran que Random Forest supera a redes neuronales en datasets estructurados (tablas) con <100,000 registros.

**Nuestro caso:**
- ~140 registros (37 meses Ã— 4 categorÃ­as - meses sin datos)
- Random Forest es ideal para este tamaÃ±o de dataset

#### âœ… **Evita overfitting**
Al combinar mÃºltiples Ã¡rboles de decisiÃ³n (ensemble learning):
```
PredicciÃ³n Final = Promedio de 100 Ã¡rboles
â†’ Reduce varianza
â†’ Generaliza mejor
```

---

## 2. Proceso de Entrenamiento

### 2.1. ObtenciÃ³n de Datos HistÃ³ricos

**Fuente:** Base de datos PostgreSQL (tabla `orders_detallepedido`)

**Rango temporal:** 
- Enero 2023 â†’ 11 Noviembre 2025 (fecha actual)
- **Nota crÃ­tica:** NO incluimos datos futuros para evitar data leakage

**Consulta SQL (simplificada):**
```sql
SELECT 
    EXTRACT(YEAR FROM p.created_at) AS aÃ±o,
    EXTRACT(MONTH FROM p.created_at) AS mes,
    c.nombre AS categoria,
    SUM(dp.cantidad) AS cantidad_vendida,
    COUNT(DISTINCT p.id) AS num_transacciones,
    AVG(dp.precio_unitario) AS precio_promedio
FROM orders_detallepedido dp
JOIN orders_pedido p ON dp.pedido_id = p.id
JOIN products_prenda pr ON dp.prenda_id = pr.id
JOIN products_categoria c ON pr.categoria_id = c.id
WHERE p.estado IN ('completado', 'entregado')
GROUP BY aÃ±o, mes, categoria
ORDER BY aÃ±o, mes, categoria;
```

**Resultado:** ~140 registros (algunos meses no tienen ventas en ciertas categorÃ­as)

---

### 2.2. AgregaciÃ³n de Datos

**âš ï¸ PUNTO CRÃTICO PARA LA DEFENSA:**

#### Â¿Por quÃ© agregamos a nivel AÃ±o-Mes-CategorÃ­a?

**ANTES (datos crudos):**
```
Pedido 1: Blusa roja, talla M, 2 unidades, 15/03/2025
Pedido 2: Blusa azul, talla S, 1 unidad, 16/03/2025
Pedido 3: Vestido, talla L, 3 unidades, 20/03/2025
...miles de registros
```

**DESPUÃ‰S (agregado):**
```
AÃ±o  Mes  CategorÃ­a   Cantidad  Transacciones  Precio_Promedio
2025  3   Blusas        233         81            35.50
2025  3   Vestidos       46         25            68.20
2025  3   Jeans         132         52            55.30
2025  3   Jackets        52         18            95.40
```

#### Ventajas de esta agregaciÃ³n:

1. **Reduce ruido:** El modelo aprende patrones generales, no fluctuaciones aleatorias de pedidos individuales
2. **Formato correcto para predicciÃ³n:** Queremos predecir "Â¿CuÃ¡ntas Blusas venderemos en Diciembre?" â†’ Necesitamos datos a nivel mensual
3. **Eficiencia:** 140 registros son mucho mÃ¡s manejables que 13,000 detalles de pedidos
4. **Evita overfitting:** Con datos tan granulares, el modelo memorizarÃ­a patrones especÃ­ficos

---

### 2.3. PreparaciÃ³n de Features

#### Features Originales ExtraÃ­das:
```python
{
    'aÃ±o': 2025,
    'mes': 3,
    'categoria': 'Blusas',
    'cantidad_vendida': 233,
    'num_transacciones': 81,
    'precio_promedio': 35.50
}
```

#### âŒ Features Descartadas

**`num_transacciones` y `precio_promedio`:**

**Â¿Por quÃ© las eliminamos?**

Estas features crean **data leakage** (filtraciÃ³n de datos):

```python
# Problema:
# Para predecir Diciembre 2025, NO conocemos:
# - Â¿CuÃ¡ntos pedidos tendremos? (num_transacciones)
# - Â¿A quÃ© precio venderemos? (precio_promedio)

# Ejemplo del problema:
PredicciÃ³n Diciembre 2025:
  Input: aÃ±o=2025, mes=12, categoria=Blusas, num_transacciones=??? 
  
# El modelo dirÃ­a: "Dame num_transacciones y te digo la cantidad"
# Pero si supiÃ©ramos num_transacciones, ya no necesitarÃ­amos predicciÃ³n!
```

**SoluciÃ³n:** Solo usar features que **conocemos con certeza en el futuro**:
- âœ… AÃ±o (2026, 2027...)
- âœ… Mes (1-12)
- âœ… CategorÃ­a (Blusas, Vestidos, Jeans, Jackets)

---

#### âœ… Feature Engineering Aplicado

##### 1. **CodificaciÃ³n de CategorÃ­as (One-Hot Encoding)**

**Problema:** El modelo no entiende texto
```python
categoria = "Blusas"  # âŒ No se puede usar directamente
```

**SoluciÃ³n: One-Hot Encoding**
```python
# TransformaciÃ³n:
Blusas   â†’ [cat_Blusas=1, cat_Vestidos=0, cat_Jeans=0, cat_Jackets=0]
Vestidos â†’ [cat_Blusas=0, cat_Vestidos=1, cat_Jeans=0, cat_Jackets=0]
Jeans    â†’ [cat_Blusas=0, cat_Vestidos=0, cat_Jeans=1, cat_Jackets=0]
Jackets  â†’ [cat_Blusas=0, cat_Vestidos=0, cat_Jeans=0, cat_Jackets=1]
```

**Ventaja:** El modelo aprende patrones especÃ­ficos por categorÃ­a:
- "Si cat_Blusas=1 â†’ ventas mÃ¡s altas"
- "Si cat_Jackets=1 â†’ ventas mÃ¡s bajas"

##### 2. **Componentes TrigonomÃ©tricas del Mes (Seasonality)**

**Problema:** El modelo ve `mes=12` y `mes=1` como nÃºmeros muy diferentes (12 vs 1)

**Realidad:** Diciembre y Enero estÃ¡n consecutivos en el ciclo anual

**SoluciÃ³n: TransformaciÃ³n trigonomÃ©trica**
```python
mes_sin = sin(2 * Ï€ * mes / 12)
mes_cos = cos(2 * Ï€ * mes / 12)
```

**VisualizaciÃ³n:**
```
Mes   mes_sin   mes_cos
1     0.50      0.87    (Enero)
2     0.87      0.50
3     1.00      0.00
...
11   -0.87     -0.50
12   -1.00      0.00   (Diciembre)
1     0.50      0.87    (Enero siguiente - cercano a Dic)
```

**Ventaja:** El modelo captura que Noviembre-Diciembre-Enero son cercanos

##### 3. **Trimestre**

Agrupa meses en 4 trimestres:
```python
Q1: Enero-Marzo (trimestre=1)
Q2: Abril-Junio (trimestre=2)
Q3: Julio-Septiembre (trimestre=3)
Q4: Octubre-Diciembre (trimestre=4)
```

**Utilidad:** Captura patrones trimestrales (ej. Q4 siempre tiene mÃ¡s ventas)

---

#### ğŸ“‹ Features Finales Utilizadas

```python
X (Features de entrada):
1. aÃ±o (2023, 2024, 2025...)
2. mes (1-12)
3. mes_sin (componente seno)
4. mes_cos (componente coseno)
5. trimestre (1-4)
6. cat_Blusas (0 o 1)
7. cat_Vestidos (0 o 1)
8. cat_Jeans (0 o 1)
9. cat_Jackets (0 o 1)

y (Target - lo que queremos predecir):
- cantidad_vendida
```

**Total:** 9 features de entrada â†’ 1 predicciÃ³n de salida

---

### 2.4. DivisiÃ³n de Datos (Train/Test)

```python
# DivisiÃ³n 80-20
X_train, X_test, y_train, y_test = train_test_split(
    X, y, 
    test_size=0.20,  # 20% para testing
    random_state=42  # Reproducibilidad
)
```

**DistribuciÃ³n:**
- **Training set:** ~112 registros (80%)
- **Test set:** ~28 registros (20%)

**PropÃ³sito:** El test set simula "datos futuros" que el modelo nunca vio durante el entrenamiento

---

### 2.5. Entrenamiento del Modelo

```python
from sklearn.ensemble import RandomForestRegressor

model = RandomForestRegressor(
    n_estimators=100,      # 100 Ã¡rboles de decisiÃ³n
    max_depth=10,          # Profundidad mÃ¡xima de cada Ã¡rbol
    min_samples_split=5,   # MÃ­nimo 5 muestras para dividir nodo
    min_samples_leaf=2,    # MÃ­nimo 2 muestras por hoja
    random_state=42        # Reproducibilidad
)

model.fit(X_train, y_train)  # Entrenamiento
```

#### HiperparÃ¡metros Explicados:

| ParÃ¡metro | Valor | Â¿QuÃ© hace? | Â¿Por quÃ© este valor? |
|-----------|-------|------------|----------------------|
| `n_estimators` | 100 | NÃºmero de Ã¡rboles | Balance entre precisiÃ³n y velocidad |
| `max_depth` | 10 | Profundidad mÃ¡xima | Evita overfitting (Ã¡rboles muy profundos memorizan) |
| `min_samples_split` | 5 | MÃ­nimo para dividir | Evita divisiones con pocos datos |
| `min_samples_leaf` | 2 | MÃ­nimo en hojas | Evita hojas con 1 solo dato (overfitting) |

---

### 2.6. EvaluaciÃ³n de Rendimiento

DespuÃ©s del entrenamiento, evaluamos el modelo:

```python
# Predicciones en test set
y_pred = model.predict(X_test)

# MÃ©tricas
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
import numpy as np

r2 = r2_score(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
```

#### Resultados TÃ­picos:

```
ğŸ“Š EVALUACIÃ“N DEL MODELO:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
RÂ² Score:        0.9727 (97.27%)
MAE:            10.34 unidades
RMSE:           15.82 unidades
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

#### InterpretaciÃ³n para la Defensa:

**RÂ² Score = 0.9727:**
- El modelo explica el **97.27%** de la variabilidad en las ventas
- Solo el **2.73%** es variaciÃ³n aleatoria que el modelo no puede capturar
- **Excelente rendimiento** (>0.90 se considera muy bueno)

**MAE = 10.34 unidades:**
- En promedio, el modelo se equivoca por Â±10 unidades
- **Ejemplo:** Si predice 200 Blusas, el valor real estarÃ¡ entre 190-210
- **Contexto:** Con ventas de 200-600 unidades/mes, un error de Â±10 es muy bajo (<5%)

**RMSE = 15.82 unidades:**
- Similar al MAE pero penaliza mÃ¡s los errores grandes
- **InterpretaciÃ³n:** Los errores son consistentes, no hay outliers grandes

---

## 3. Importancia de Features (Feature Importance)

```python
feature_importance = pd.DataFrame({
    'feature': feature_names,
    'importance': model.feature_importances_
}).sort_values('importance', ascending=False)
```

### Resultados TÃ­picos:

```
Feature          Importance    InterpretaciÃ³n
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cat_Blusas       0.5823 (58%)  â† CategorÃ­a mÃ¡s influyente
cat_Jeans        0.1845 (18%)  â† Segunda categorÃ­a importante
mes              0.0932 (9%)   â† Estacionalidad
aÃ±o              0.0654 (7%)   â† Tendencia temporal
cat_Vestidos     0.0417 (4%)   â† CategorÃ­a de menor volumen
trimestre        0.0198 (2%)   â† AgrupaciÃ³n temporal
mes_sin          0.0087 (1%)   â† Componente sinusoidal
mes_cos          0.0036 (<1%)  â† Componente cosinusoidal
cat_Jackets      0.0008 (<1%)  â† CategorÃ­a con menos datos
```

### ğŸ¯ InterpretaciÃ³n para la Defensa:

#### 1. **cat_Blusas domina (58%)**

**Â¿QuÃ© significa?**
- La categorÃ­a del producto es el **factor mÃ¡s determinante** del volumen de ventas
- Las Blusas representan ~50% del volumen total de ventas

**Â¿Por quÃ©?**
- Producto estrella del negocio
- Mayor variedad (tenemos 2000 blusas vs 500 vestidos en inventario)
- Precio mÃ¡s accesible â†’ mÃ¡s transacciones

**Para la defensa:**
> "El modelo identificÃ³ que la categorÃ­a 'Blusas' es el predictor mÃ¡s importante (58%), lo cual es coherente con nuestros datos: las Blusas representan el 50% de las ventas totales. Esto valida que el modelo estÃ¡ aprendiendo patrones reales del negocio."

#### 2. **mes es importante (9%)**

**Â¿QuÃ© significa?**
- El mes del aÃ±o influye significativamente en las ventas
- Captura **estacionalidad** (ej. Noviembre-Diciembre ventas altas)

**Evidencia:**
```
Mes      Ventas Promedio
Nov      1,369 unidades  â† Pico
Dic      1,228 unidades  â† Pico
Mar        463 unidades  â† Normal
Feb        546 unidades  â† Normal
```

**Para la defensa:**
> "El modelo asigna 9% de importancia al mes, capturando la estacionalidad del negocio. Observamos picos de ventas en Noviembre-Diciembre debido a fiestas de fin de aÃ±o."

#### 3. **aÃ±o tambiÃ©n importa (7%)**

**Â¿QuÃ© significa?**
- Hay una **tendencia de crecimiento** aÃ±o tras aÃ±o

**Evidencia:**
```
AÃ±o   Ventas Totales
2023   6,105 unidades
2024   6,563 unidades (+7.5%)
2025   7,144 unidades (+8.9%)
```

**Para la defensa:**
> "El modelo detecta una tendencia de crecimiento anual del ~8%, reflejando la expansiÃ³n del negocio y aumento de la base de clientes."

#### 4. **cat_Jackets tiene baja importancia (<1%)**

**Â¿Significa que el modelo no predice Jackets?**
- âŒ **NO**. El modelo SÃ predice Jackets correctamente
- âœ… La baja importancia significa que **Jackets sigue patrones similares a otras categorÃ­as**

**AnalogÃ­a:**
```
Si todas las categorÃ­as crecen 10% en Noviembre:
â†’ No necesitas "categoria" para predecir
â†’ Solo necesitas "mes=Noviembre"

Pero si Blusas crecen 50% y Jackets solo 5%:
â†’ AhÃ­ sÃ­ necesitas saber la categorÃ­a
```

**Para la defensa:**
> "La baja importancia de Jackets no indica falta de predicciÃ³n, sino que sus patrones de venta son mÃ¡s uniformes y predecibles usando solo variables temporales (mes, aÃ±o)."

---

## 4. Ventajas del Modelo Unificado

### Â¿Por quÃ© NO entrenar 4 modelos separados?

#### OpciÃ³n A: 4 Modelos Separados âŒ
```python
modelo_blusas.fit(datos_blusas)      # 37 registros
modelo_vestidos.fit(datos_vestidos)  # 37 registros
modelo_jeans.fit(datos_jeans)        # 37 registros
modelo_jackets.fit(datos_jackets)    # 37 registros
```

**Desventajas:**
1. **Menos datos por modelo:** 37 registros es muy poco â†’ alto riesgo de overfitting
2. **4Ã— mÃ¡s trabajo:** Entrenar, evaluar y mantener 4 modelos
3. **DifÃ­cil comparaciÃ³n:** No puedes comparar patrones entre categorÃ­as
4. **No aprende de similitudes:** Si Jeans y Blusas tienen patrones similares, cada modelo lo aprende desde cero

#### OpciÃ³n B: 1 Modelo Unificado âœ…
```python
modelo_unico.fit(datos_todas_categorias)  # 140 registros
# PredicciÃ³n: model.predict([aÃ±o, mes, categoria])
```

**Ventajas:**
1. **MÃ¡s datos:** 140 registros â†’ mejor generalizaciÃ³n
2. **Mantenibilidad:** Un solo modelo para entrenar/actualizar
3. **Aprende similitudes:** Si 3 categorÃ­as tienen pico en Diciembre, el modelo lo usa para predecir la 4ta
4. **Escalabilidad:** Agregar una categorÃ­a nueva es trivial (solo agregar columna one-hot)

**Para la defensa:**
> "Elegimos un modelo unificado porque permite aprovechar patrones comunes entre categorÃ­as (ej. estacionalidad), maximiza el uso de datos disponibles (140 vs 37 registros), y facilita el mantenimiento a largo plazo."

---

## 5. Flujo Completo del Sistema

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. DATOS CRUDOS (PostgreSQL)                                â”‚
â”‚    - 13,020 registros de ventas                             â”‚
â”‚    - Pedidos de Ene 2023 â†’ Nov 2025                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. AGREGACIÃ“N (DataPreparationService)                      â”‚
â”‚    - Agrupar por AÃ±o-Mes-CategorÃ­a                          â”‚
â”‚    - Resultado: ~140 registros                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. FEATURE ENGINEERING                                       â”‚
â”‚    - One-Hot Encoding (categorÃ­as)                          â”‚
â”‚    - Componentes trigonomÃ©tricas (mes)                      â”‚
â”‚    - Features finales: 9 columnas                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. ENTRENAMIENTO (ModelTrainingService)                     â”‚
â”‚    - Random Forest (100 Ã¡rboles)                            â”‚
â”‚    - Train/Test split (80/20)                               â”‚
â”‚    - EvaluaciÃ³n: RÂ²=97.27%, MAE=10.34                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. PERSISTENCIA                                              â”‚
â”‚    - Guardar modelo.pkl                                      â”‚
â”‚    - Guardar metadata.json                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. PREDICCIÃ“N (PredictionService)                           â”‚
â”‚    - Cargar modelo.pkl                                       â”‚
â”‚    - Predecir prÃ³ximos N meses                              â”‚
â”‚    - Generar dashboard JSON                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 7. VISUALIZACIÃ“N (Frontend React)                           â”‚
â”‚    - GrÃ¡ficos interactivos                                   â”‚
â”‚    - Filtros dinÃ¡micos                                       â”‚
â”‚    - InterpretaciÃ³n de negocio                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 6. Casos de Uso y Valor de Negocio

### Ejemplo Real: PredicciÃ³n para Diciembre 2025

**Input al modelo:**
```python
{
    'aÃ±o': 2025,
    'mes': 12,
    'cat_Blusas': 1,
    'cat_Vestidos': 0,
    'cat_Jeans': 0,
    'cat_Jackets': 0,
    # ... otras features calculadas automÃ¡ticamente
}
```

**Output del modelo:**
```
Blusas Diciembre 2025: 638 unidades (Â±10)
```

**Decisiones de negocio:**
1. **Inventario:** Stockear 650 Blusas para Diciembre
2. **Compras:** Contactar proveedores en Octubre-Noviembre
3. **Marketing:** Preparar campaÃ±as para Blusas en Diciembre
4. **Staff:** Contratar personal temporal para atender demanda

---

## 7. Limitaciones y Mejoras Futuras

### Limitaciones Actuales:

1. **No considera eventos externos:**
   - Promociones especiales
   - DÃ­as festivos locales
   - Competencia

2. **Datos limitados a 3 aÃ±os:**
   - Con mÃ¡s aÃ±os, capturarÃ­a mejor las tendencias

3. **No considera precio dinÃ¡mico:**
   - Asume precios estables

### Mejoras Futuras:

1. **Agregar features externas:**
   ```python
   - es_promocion (0/1)
   - dias_festivos_mes (cantidad)
   - temperatura_promedio
   ```

2. **Reentrenar periÃ³dicamente:**
   - Cada 3 meses con nuevos datos
   - Ajustar hiperparÃ¡metros

3. **A/B Testing:**
   - Comparar predicciones vs ventas reales
   - Medir ROI de decisiones basadas en IA

---

## 8. ConclusiÃ³n

El modelo de Random Forest implementado en SmartSales365 demuestra:

âœ… **Alta precisiÃ³n:** RÂ² = 97.27%  
âœ… **Interpretabilidad:** Feature importance clara  
âœ… **Robustez:** Maneja estacionalidad y tendencias  
âœ… **Escalabilidad:** FÃ¡cil agregar nuevas categorÃ­as  
âœ… **Valor de negocio:** Optimiza inventario y reduce costos  

**El modelo estÃ¡ listo para producciÃ³n y toma de decisiones estratÃ©gicas.**

---

**Ãšltima actualizaciÃ³n:** 11 de Noviembre de 2025  
**VersiÃ³n del modelo:** v1.0_20251111  
**PrÃ³xima revisiÃ³n:** Febrero 2026
